{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Problem 1 - Learning and Estimation",
   "id": "58843d77d80e129a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-20T10:41:50.225560Z",
     "start_time": "2024-12-20T10:41:49.773829Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# Function to preprocess images (both images and labels)\n",
    "def preprocess_images(image_path, target_size=(256, 256)):\n",
    "    images = []\n",
    "    for file_name in sorted(os.listdir(image_path)):\n",
    "        image = load_img(os.path.join(image_path, file_name), target_size=target_size)\n",
    "        image = img_to_array(image) / 255.0  # Normalize the image\n",
    "        images.append(image)\n",
    "    return np.array(images)\n",
    "\n",
    "# Function to load images and labels\n",
    "def load_data(images_dir, labels_dir, image_size=(256, 256)):\n",
    "    # Load the images and labels\n",
    "    images = preprocess_images(images_dir, image_size)\n",
    "    labels = preprocess_images(labels_dir, image_size)\n",
    "\n",
    "    # Check if the arrays are empty\n",
    "    if images.size == 0:\n",
    "        raise ValueError(\"Error: No images found in the directory\")\n",
    "    if labels.size == 0:\n",
    "        raise ValueError(\"Error: No labels found in the directory\")\n",
    "\n",
    "    # Check if images and labels have the same number of elements\n",
    "    if images.shape[0] != labels.shape[0]:\n",
    "        raise ValueError(\"Error: Number of images and labels do not match\")\n",
    "\n",
    "    # Expand the dimensions of labels if they are grayscale images (3D array)\n",
    "    if len(labels.shape) == 3:  # If labels are grayscale (3D array)\n",
    "        labels = np.expand_dims(labels, axis=-1)  # Add a channel dimension to labels (4D array)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "# Path to dataset\n",
    "train_images_path = \"data/membrane/train/image\"\n",
    "train_labels_path = \"data/membrane/train/label\"\n",
    "test_images_path = \"data/membrane/test\"\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    train_images, train_labels = load_data(train_images_path, train_labels_path)\n",
    "    print(f\"train_images shape: {train_images.shape}\")\n",
    "    print(f\"train_labels shape: {train_labels.shape}\")\n",
    "\n",
    "except ValueError as e:\n",
    "    print(e)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images shape: (30, 256, 256, 3)\n",
      "train_labels shape: (30, 256, 256, 3)\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-20T11:23:02.638987Z",
     "start_time": "2024-12-20T10:59:27.066190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Flatten, Dense, Reshape, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Assuming train_images and val_images are already loaded as numpy arrays and have shape (num_samples, 256, 256, 3)\n",
    "\n",
    "# Normalize the images to the range [0, 1]\n",
    "train_images = train_images / 255.0\n",
    "val_images = val_images / 255.0\n",
    "\n",
    "# Assuming you have corresponding labels for the images\n",
    "train_labels = train_labels / 255.0\n",
    "val_labels = val_labels / 255.0\n",
    "\n",
    "# Define the model architecture\n",
    "input_layer = Input(shape=(256, 256, 3))  # Input shape for RGB images\n",
    "model = Sequential()\n",
    "\n",
    "# Contracting Path (Encoder)\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(256, 256, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Bottom (Bottleneck)\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "\n",
    "# Expanding Path (Decoder)\n",
    "model.add(UpSampling2D(size=(2, 2)))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "\n",
    "model.add(UpSampling2D(size=(2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "\n",
    "model.add(UpSampling2D(size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "\n",
    "model.add(UpSampling2D(size=(2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "\n",
    "# Output layer\n",
    "model.add(Conv2D(3, (3, 3), activation='sigmoid', padding='same'))  # Output has 3 channels for RGB\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Fit the model using the augmented training data\n",
    "model.fit(\n",
    "    datagen.flow(train_images, train_labels, batch_size=32),\n",
    "    epochs=50,  # Number of epochs, adjust as needed\n",
    "    validation_data=(val_images, val_labels)\n",
    ")\n",
    "\n",
    "# Save the trained model after training\n",
    "model.save('unet_model.h5')\n",
    "\n",
    "# You can later load the model using:\n",
    "model = tf.keras.models.load_model('unet_model.h5')\n"
   ],
   "id": "aa4bbe06fe6b80da",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m42s\u001B[0m 42s/step - accuracy: 1.0000 - loss: 0.2500 - val_accuracy: 0.0000e+00 - val_loss: 0.2487\n",
      "Epoch 2/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m34s\u001B[0m 34s/step - accuracy: 0.0000e+00 - loss: 0.2487 - val_accuracy: 0.0000e+00 - val_loss: 0.2442\n",
      "Epoch 3/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m34s\u001B[0m 34s/step - accuracy: 0.0000e+00 - loss: 0.2442 - val_accuracy: 0.9370 - val_loss: 0.2220\n",
      "Epoch 4/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m29s\u001B[0m 29s/step - accuracy: 0.9370 - loss: 0.2220 - val_accuracy: 0.9491 - val_loss: 0.1318\n",
      "Epoch 5/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m24s\u001B[0m 24s/step - accuracy: 0.9491 - loss: 0.1318 - val_accuracy: 0.0327 - val_loss: 0.0110\n",
      "Epoch 6/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m34s\u001B[0m 34s/step - accuracy: 0.0327 - loss: 0.0110 - val_accuracy: 7.6294e-05 - val_loss: 7.5813e-04\n",
      "Epoch 7/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m31s\u001B[0m 31s/step - accuracy: 7.6294e-05 - loss: 7.5813e-04 - val_accuracy: 1.5259e-05 - val_loss: 4.3157e-05\n",
      "Epoch 8/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m27s\u001B[0m 27s/step - accuracy: 1.5259e-05 - loss: 4.3157e-05 - val_accuracy: 0.0000e+00 - val_loss: 4.1204e-06\n",
      "Epoch 9/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m25s\u001B[0m 25s/step - accuracy: 0.0000e+00 - loss: 4.1204e-06 - val_accuracy: 0.9095 - val_loss: 4.6552e-07\n",
      "Epoch 10/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m27s\u001B[0m 27s/step - accuracy: 0.9095 - loss: 4.6552e-07 - val_accuracy: 0.9655 - val_loss: 1.9111e-08\n",
      "Epoch 11/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m27s\u001B[0m 27s/step - accuracy: 0.9655 - loss: 1.9111e-08 - val_accuracy: 0.9826 - val_loss: 1.7197e-10\n",
      "Epoch 12/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m28s\u001B[0m 28s/step - accuracy: 0.9826 - loss: 1.7197e-10 - val_accuracy: 0.9911 - val_loss: 2.8348e-13\n",
      "Epoch 13/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m26s\u001B[0m 26s/step - accuracy: 0.9911 - loss: 2.8348e-13 - val_accuracy: 0.9984 - val_loss: 8.2313e-17\n",
      "Epoch 14/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m27s\u001B[0m 27s/step - accuracy: 0.9984 - loss: 8.2313e-17 - val_accuracy: 0.9993 - val_loss: 3.9485e-21\n",
      "Epoch 15/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m24s\u001B[0m 24s/step - accuracy: 0.9993 - loss: 3.9485e-21 - val_accuracy: 0.9997 - val_loss: 2.9740e-26\n",
      "Epoch 16/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m27s\u001B[0m 27s/step - accuracy: 0.9997 - loss: 2.9740e-26 - val_accuracy: 0.9998 - val_loss: 3.1050e-32\n",
      "Epoch 17/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m27s\u001B[0m 27s/step - accuracy: 0.9998 - loss: 3.1050e-32 - val_accuracy: 0.9999 - val_loss: 0.0000e+00\n",
      "Epoch 18/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m29s\u001B[0m 29s/step - accuracy: 0.9999 - loss: 0.0000e+00 - val_accuracy: 0.9999 - val_loss: 0.0000e+00\n",
      "Epoch 19/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m30s\u001B[0m 30s/step - accuracy: 0.9999 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 20/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m38s\u001B[0m 38s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 21/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 32s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 22/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m26s\u001B[0m 26s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 23/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 23s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 24/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 23s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 25/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m29s\u001B[0m 29s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 26/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 32s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 27/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m34s\u001B[0m 34s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 28/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m27s\u001B[0m 27s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 29/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m24s\u001B[0m 24s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 30/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 23s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 31/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 23s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 32/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m38s\u001B[0m 38s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 33/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m21s\u001B[0m 21s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 34/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m21s\u001B[0m 21s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 35/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m27s\u001B[0m 27s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 36/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m33s\u001B[0m 33s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 37/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m50s\u001B[0m 50s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 38/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m46s\u001B[0m 46s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 39/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m30s\u001B[0m 30s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 40/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m29s\u001B[0m 29s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 41/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m28s\u001B[0m 28s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 42/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m22s\u001B[0m 22s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 43/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m22s\u001B[0m 22s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 44/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m22s\u001B[0m 22s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 45/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m22s\u001B[0m 22s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 46/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 23s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 47/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 23s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 48/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 23s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 49/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 23s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 50/50\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m22s\u001B[0m 22s/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Problem 2 - Code Reading",
   "id": "cfe6a4861a36fea8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Summary of \"U-Net: Convolutional Networks for Biomedical Image Segmentation\"\n",
    "\n",
    "The U-Net paper introduces a convolutional neural network architecture that is especially designed for biomedical image segmentation. Here's a breakdown of the key elements:\n",
    "\n",
    "Architecture Overview:\n",
    "\n",
    "Contracting Path (Encoder): This part of the network uses convolution and max-pooling layers to capture context and reduce the spatial dimensions of the input image. It essentially extracts features from the image.\n",
    "\n",
    "Bottleneck: This is where the model processes the most abstract features, often with smaller spatial dimensions.\n",
    "\n",
    "Expansive Path (Decoder): This part of the network uses transposed convolutions (up-sampling) to recover spatial dimensions and refine the output by adding context back into the segmented regions. The expansive path essentially \"decodes\" the high-level features into pixel-wise segmentation maps.\n",
    "\n",
    "Skip Connections: These connections directly pass feature maps from the contracting path to the corresponding layers in the expansive path, which helps to recover fine-grained details during decoding. This helps U-Net avoid losing spatial information during down-sampling.\n",
    "\n",
    "Final Output:\n",
    "\n",
    "U-Net produces pixel-wise binary classification maps (for binary segmentation tasks), with the final layer typically being a softmax or sigmoid activation function for segmentation tasks.\n",
    "Loss Function:\n",
    "\n",
    "The most common loss function for U-Net is binary cross-entropy for binary segmentation or categorical cross-entropy for multi-class segmentation tasks.\n",
    "Training Strategy:\n",
    "\n",
    "U-Net is often trained on a relatively small number of images, relying on data augmentation techniques like rotations, flips, and scaling to increase the variety of training data.\n",
    "\n",
    "\n",
    "Applicability:\n",
    "\n",
    "While originally designed for biomedical image segmentation, U-Net has been successfully applied in many other domains, such as satellite image segmentation, road detection, and more.\n",
    "\n",
    "Code Walkthrough Based on U-Net Architecture\n",
    "\n",
    "1. Model Architecture:\n",
    "Input Layer: The input to the network is a 2D or 3D image.\n",
    "Encoder: The encoder typically consists of convolutional layers followed by max-pooling operations to downsample the input.\n",
    "Bottleneck: After downsampling, the network processes the most abstract features at a bottleneck point, where the spatial dimensions are reduced.\n",
    "2. \n",
    "Decoder: Transposed convolution layers (also called deconvolution layers) are used to upsample the data and recover spatial resolution.\n",
    "Skip Connections: These connections pass features from the encoder directly to the corresponding decoder layers."
   ],
   "id": "ff61b7914dd4f562"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
